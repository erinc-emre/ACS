{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a421e6",
   "metadata": {},
   "source": [
    "# Advanced Commit Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fde77",
   "metadata": {},
   "source": [
    "#### Env Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3d2a9",
   "metadata": {},
   "source": [
    "These configuration variables control the behavior of the vector database operations:\n",
    "\n",
    "- **VECTOR_DB_INSERT_BATCH_SIZE**: Number of vectors to insert into Qdrant in each batch operation\n",
    "\n",
    "These constants help optimize performance and manage resource usage throughout the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0c79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB_INSERT_BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb12351",
   "metadata": {},
   "source": [
    "#### Imports and Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbc271",
   "metadata": {},
   "source": [
    "This section imports all the necessary libraries and downloads required NLTK data:\n",
    "\n",
    "**Core Libraries:**\n",
    "- `re`: Regular expressions for text processing\n",
    "- `nltk`: Natural Language Toolkit for sentence tokenization\n",
    "- `xml.etree.ElementTree`: XML parsing capabilities\n",
    "- `tqdm`: Progress bars for long-running operations\n",
    "- `os`: File system operations\n",
    "- `sqlite3`: SQLite database operations\n",
    "- `subprocess`: Running shell commands\n",
    "\n",
    "**Machine Learning & Vector Database:**\n",
    "- `sentence_transformers`: Convert text to semantic embeddings\n",
    "- `qdrant_client`: Vector database for similarity search\n",
    "- `qdrant_client.models`: Data structures for vector operations\n",
    "\n",
    "**NLTK Downloads:**\n",
    "- `punkt`: Sentence tokenizer models\n",
    "- `punkt_tab`: Additional tokenization data\n",
    "\n",
    "These libraries enable the complete pipeline from data extraction to semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af99947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "import sqlite3\n",
    "import subprocess\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadcb414",
   "metadata": {},
   "source": [
    "## 1- Download Commit Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18336bf",
   "metadata": {},
   "source": [
    "This section executes the shell script to download commit messages from Git repositories:\n",
    "\n",
    "**Process Flow:**\n",
    "1. **Script Execution**: Runs `export_commit_messages.sh` using subprocess\n",
    "2. **Real-time Output**: Streams stdout in real-time to show progress\n",
    "3. **Error Handling**: Captures and displays any errors from stderr\n",
    "4. **Exit Validation**: Checks return code to ensure successful completion\n",
    "\n",
    "The script downloads commit data and converts it to XML format for further processing. This step is essential for gathering the raw data that will be processed into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(['bash', 'export_commit_messages.sh'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "if process.returncode != 0:\n",
    "    stderr = process.stderr.read()\n",
    "    print(\"Error downloading commit messages:\", stderr)\n",
    "    exit(1)\n",
    "print(\"Commit messages downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884de9c",
   "metadata": {},
   "source": [
    "## 2- Validate XML Files with XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ba354",
   "metadata": {},
   "source": [
    "! You will see an error in carbon reposity at commit afa265dd5710a81bda1ad81fc992f5441e9a15da due to invalid characters in the commit message. Remove the invalid characters and re-run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4ac36",
   "metadata": {},
   "source": [
    "This step validates the downloaded XML files against a predefined XSD schema:\n",
    "\n",
    "**Validation Process:**\n",
    "1. **Schema Validation**: Uses `validate-xml.sh` to check XML structure\n",
    "2. **Error Detection**: Identifies malformed XML or invalid characters\n",
    "3. **Data Quality**: Ensures XML files meet expected format requirements\n",
    "4. **Pre-processing Check**: Validates data before proceeding to parsing\n",
    "\n",
    "**Common Issues:**\n",
    "- Invalid characters in commit messages (like control characters)\n",
    "- Malformed XML structure\n",
    "- Encoding problems\n",
    "\n",
    "XML validation is crucial for preventing parsing errors in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9467f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mXML Validation Script\u001b[0m\n",
      "=============================\n",
      "\u001b[1;33mSchema file:\u001b[0m repository-commit-schema.xsd\n",
      "\u001b[1;33mXML directory:\u001b[0m XML_commit_messages\n",
      "\n",
      "\u001b[1;33mValidating schema file...\u001b[0m\n",
      "\u001b[0;32m✓ Schema file is valid\u001b[0m\n",
      "\n",
      "\u001b[1;33mValidating XML files...\u001b[0m\n",
      "\n",
      "Validating carbon-lang_commits.xml... \u001b[0;32m✓ VALID\u001b[0m\n",
      "Validating pyrefly_commits.xml... \u001b[0;32m✓ VALID\u001b[0m\n",
      "\n",
      "\u001b[1;33mValidation Summary:\u001b[0m\n",
      "\u001b[1;33m  Files found:\u001b[0m 2\n",
      "\u001b[0;32m  Valid files:\u001b[0m 2\n",
      "\u001b[0;31m  Invalid files:\u001b[0m 0\n",
      "\n",
      "\u001b[0;32m✓ All XML files are valid according to the schema\u001b[0m\n",
      "\n",
      "XML file validated successfully.\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(['bash', 'validate-xml.sh'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(\"Error validating XML file:\", result.stderr)\n",
    "    exit(1)\n",
    "print(\"XML file validated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8ff75",
   "metadata": {},
   "source": [
    "## 3- Create Qdrant Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77db7b",
   "metadata": {},
   "source": [
    "### Paragraph to Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4c24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_to_sentences(paragraph):\n",
    "    # Split by two or more consecutive newlines\n",
    "    parts = re.split(r'\\n{2,}', paragraph)\n",
    "    sentences = []\n",
    "    for part in parts:\n",
    "        # Replace single newlines with spaces\n",
    "        part = part.replace('\\n', ' ')\n",
    "        sentences.extend(sent_tokenize(part))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8f1b7",
   "metadata": {},
   "source": [
    "This function processes commit messages that may contain multiple paragraphs or sentences:\n",
    "\n",
    "1. **Split paragraphs**: Uses regex to split on two or more consecutive newlines\n",
    "2. **Normalize whitespace**: Replaces single newlines with spaces within paragraphs\n",
    "3. **Sentence tokenization**: Uses NLTK to properly split text into individual sentences\n",
    "\n",
    "This preprocessing improves the quality of embeddings by creating more focused, sentence-level chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55f04f",
   "metadata": {},
   "source": [
    "### XML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a79dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_commits(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    repo = root.find('repository')\n",
    "    repo_url = repo.find('url').text if repo is not None else None\n",
    "    repo_name = repo.find('name').text if repo is not None else None\n",
    "    commits = []\n",
    "    for commit in tqdm(root.findall('.//commit'), desc=\"Parsing commits\"):\n",
    "        message = commit.find('message').text\n",
    "        author = commit.find('author').text\n",
    "        date = commit.find('date').text\n",
    "        hash = commit.find('hash').text\n",
    "        commits.append({\"message\": message,\n",
    "                        \"author\": author,\n",
    "                        \"date\": date,\n",
    "                        \"hash\": hash,\n",
    "                        \"repo_url\": repo_url,\n",
    "                        \"repo_name\": repo_name})\n",
    "    return commits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e57870",
   "metadata": {},
   "source": [
    "Parse XML files containing commit data and extract structured information:\n",
    "\n",
    "1. **XML parsing**: Use ElementTree to parse the XML structure\n",
    "2. **Repository metadata**: Extract repository URL and name from the root element\n",
    "3. **Commit iteration**: Loop through all commit elements with a progress bar\n",
    "4. **Data extraction**: Extract message, author, date, and hash for each commit\n",
    "5. **Return structured data**: Create a list of dictionaries with all commit information\n",
    "\n",
    "The function returns a standardized format that can be easily processed by other parts of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d391c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing commits: 100%|██████████| 6531/6531 [00:00<00:00, 943155.19it/s]\n",
      "Parsing commits: 100%|██████████| 4182/4182 [00:00<00:00, 1011100.95it/s]\n"
     ]
    }
   ],
   "source": [
    "commits = []\n",
    "xml_dir = 'XML_commit_messages'\n",
    "for filename in os.listdir(xml_dir):\n",
    "    if filename.endswith('.xml'):\n",
    "        file_path = os.path.join(xml_dir, filename)\n",
    "        commits.extend(parse_commits(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5c5e0",
   "metadata": {},
   "source": [
    "Process all XML files in the XML_commit_messages directory and combine their commit data into a single list. This allows us to work with commits from multiple repositories in a unified way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f18188",
   "metadata": {},
   "source": [
    "**Multi-Repository Processing:**\n",
    "\n",
    "This code iterates through all XML files in the `XML_commit_messages` directory and processes them:\n",
    "\n",
    "1. **Directory Scanning**: Lists all files in the XML directory\n",
    "2. **File Filtering**: Only processes files with `.xml` extension\n",
    "3. **Batch Processing**: Calls `parse_commits()` for each XML file\n",
    "4. **Data Aggregation**: Combines commits from all repositories into a single list\n",
    "\n",
    "This approach allows the system to handle multiple Git repositories simultaneously, creating a unified dataset for analysis. Each repository's commits are parsed and added to the master `commits` list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cee19",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bbf0cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model multi-qa-MiniLM-L6-cos-v1 ...\n",
      "Generating embeddings...\n"
     ]
    }
   ],
   "source": [
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "print(f\"Loading model {model_name} ...\")\n",
    "model = SentenceTransformer(model_name)  # better for query → doc\n",
    "print(\"Generating embeddings...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b98e3d",
   "metadata": {},
   "source": [
    "Load the SentenceTransformer model for generating embeddings:\n",
    "\n",
    "- **Model choice**: `multi-qa-MiniLM-L6-cos-v1` is optimized for query-to-document similarity\n",
    "- **Size**: Produces 384-dimensional vectors\n",
    "- **Performance**: Good balance between speed and quality for semantic search tasks\n",
    "\n",
    "This model will convert commit messages into numerical vectors that capture their semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5912434",
   "metadata": {},
   "source": [
    "### Set Up Qdrant & Store Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b62ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Upserting to Qdrant: 100%|██████████| 108/108 [00:03<00:00, 35.88it/s]\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(host='qdrant', port=6333)\n",
    "\n",
    "if not client.collection_exists(collection_name=\"commits\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"commits\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    client.delete_collection(collection_name=\"commits\")\n",
    "    client.create_collection(\n",
    "        collection_name=\"commits\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "\n",
    "points = [PointStruct(id=i, vector=model.encode(c['message'], convert_to_numpy=True).tolist(), payload={\"commit-hash\": c['hash'], \"author\": c['author'], \"date\": c['date'], \"message\": c['message']})\n",
    "          for i, c in enumerate(commits)]\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(points), VECTOR_DB_INSERT_BATCH_SIZE), desc=\"Upserting to Qdrant\"):\n",
    "    batch = points[i:i+VECTOR_DB_INSERT_BATCH_SIZE]\n",
    "    client.upsert(collection_name=\"commits\", points=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7659a2",
   "metadata": {},
   "source": [
    "Set up the Qdrant vector database and store commit embeddings:\n",
    "\n",
    "1. **Collection management**: Create or recreate the \"commits\" collection\n",
    "2. **Vector configuration**: 384-dimensional vectors with cosine similarity\n",
    "3. **Embedding generation**: Convert each commit message to a vector\n",
    "4. **Batch insertion**: Insert vectors in batches for better performance\n",
    "5. **Metadata storage**: Store commit hash, author, date, and message as payload\n",
    "\n",
    "This creates a searchable vector database where we can find semantically similar commits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fe6b4",
   "metadata": {},
   "source": [
    "### Embed User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a2da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_user_query(query):\n",
    "    return model.encode(query, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e59f6",
   "metadata": {},
   "source": [
    "Convert user search queries into embeddings using the same model used for commit messages. This ensures that queries and documents exist in the same vector space for accurate similarity comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9cd9f",
   "metadata": {},
   "source": [
    "### Search Qdrant for Similar Commit Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "403d7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_commits(query, result_limit, score_threshold=0.5):\n",
    "    vector = embed_user_query(query)\n",
    "    results = client.query_points(\n",
    "        collection_name=\"commits\",\n",
    "        query=vector.tolist(),\n",
    "        limit=result_limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    if not results.points:\n",
    "        return \"No similar commit found.\"\n",
    "    \n",
    "    # Filter results by score threshold\n",
    "    filtered_results = [res for res in results.points if res.score > score_threshold]\n",
    "    if not filtered_results:\n",
    "        return \"No similar commit found above the score threshold.\"\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93d9a4",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd703be",
   "metadata": {},
   "source": [
    "**Semantic Search Function:**\n",
    "\n",
    "This function performs vector-based similarity search on commit messages:\n",
    "\n",
    "**Process Flow:**\n",
    "1. **Query Embedding**: Convert the user's search query into a vector using the same model\n",
    "2. **Vector Search**: Use Qdrant's `query_points` method to find the most similar commit message vectors\n",
    "3. **Result Limiting**: Return only the top N results\n",
    "4. **Payload Inclusion**: Include commit metadata (hash, author, date, message) in results\n",
    "5. **Score Threshold**: Filter results to only include those above a configurable similarity score\n",
    "6. **Fallback**: Return helpful message if no similar commits are found\n",
    "\n",
    "**Key Features:**\n",
    "- Uses cosine similarity to measure semantic closeness\n",
    "- Returns structured results with similarity scores\n",
    "- Maintains consistent vector space between queries and documents\n",
    "- Score threshold can be adjusted for more/less strict matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8875a4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=10116, version=101, score=0.6703605, payload={'commit-hash': '70a7839a3199e044f391573a184d050beff095ab', 'author': 'gromer@google.com', 'date': '2021-10-27 17:09:03 -0700', 'message': 'Fix bug from #909 (#924)'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=9565, version=95, score=0.56431377, payload={'commit-hash': '4192ee42a6e8aab7cfa60d5d8328df83a8032790', 'author': 'josh11b@users.noreply.github.com', 'date': '2022-08-30 08:25:24 -0700', 'message': 'Misc small fixes (#2123)'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=10426, version=104, score=0.5512945, payload={'commit-hash': '82f5c4224b25f793619585786f0368915e96479c', 'author': '46229924+jonmeow@users.noreply.github.com', 'date': '2021-05-14 13:23:10 -0700', 'message': 'Small fixes from #530 (#537)'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=9720, version=97, score=0.52749014, payload={'commit-hash': '8ed38c42f398c8051cdb833c4f621d0b96e56732', 'author': 'sota.monokuma.bass.1@gmail.com', 'date': '2022-07-21 03:55:54 +0900', 'message': 'Fixed typo (#1473)'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=9711, version=97, score=0.51892394, payload={'commit-hash': '3f247037ad9cdf0cf4a8a56dad9aae83e93c053b', 'author': 'aleksey.kladov@gmail.com', 'date': '2022-07-21 11:12:26 +0100', 'message': 'fix typo (#1505)'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=8686, version=86, score=0.5169556, payload={'commit-hash': 'e7b3d395b37b1ab87303092d3570bbb1b594533b', 'author': 'josh11b@users.noreply.github.com', 'date': '2023-10-26 13:59:28 -0700', 'message': 'Fix typo (#3340)'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search_similar_commits(\"bug fix\",result_limit=10, score_threshold=0.5)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f55b6",
   "metadata": {},
   "source": [
    "**Demo Search Query:**\n",
    "\n",
    "This example demonstrates the semantic search functionality by searching for commits related to \"bug fix\":\n",
    "\n",
    "**What This Does:**\n",
    "- Converts \"bug fix\" into a vector representation\n",
    "- Uses Qdrant's `query_points` method to search the commit database for semantically similar messages\n",
    "- Returns the top 10 most similar commits with their metadata\n",
    "- Filters results using the score threshold (0.5) to ensure quality matches\n",
    "- Shows similarity scores indicating how closely each commit matches the query\n",
    "\n",
    "**Expected Results:**\n",
    "- Commits containing words like \"fix\", \"bug\", \"issue\", \"resolve\"\n",
    "- Commits with similar semantic meaning even without exact word matches\n",
    "- Results ranked by semantic similarity score\n",
    "- Only commits with similarity scores above 0.5 are returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ab86c",
   "metadata": {},
   "source": [
    "## 4- Create a SQLite Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea652f52",
   "metadata": {},
   "source": [
    "### SQLite Database Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a97cf",
   "metadata": {},
   "source": [
    "This section creates a SQLite database to store commit messages in a relational format. The database will have two tables:\n",
    "- `repositories`: stores repository information (URL and name)\n",
    "- `commits`: stores commit details with a foreign key reference to repositories\n",
    "\n",
    "SQLite provides ACID transactions and allows us to perform complex queries on the commit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf3309f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('commit_messages.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe83ee1",
   "metadata": {},
   "source": [
    "**Database Connection Setup:**\n",
    "\n",
    "This code establishes a connection to the SQLite database:\n",
    "\n",
    "- **Database File**: Creates or connects to `commit_messages.db`\n",
    "- **Auto-Creation**: SQLite automatically creates the file if it doesn't exist\n",
    "- **Cursor Object**: Provides an interface to execute SQL commands\n",
    "- **Local Storage**: Stores data persistently on the local file system\n",
    "\n",
    "SQLite is chosen for its simplicity, zero-configuration setup, and ability to handle the commit data efficiently without requiring a separate database server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658516e",
   "metadata": {},
   "source": [
    "Create a connection to the SQLite database file. If the file doesn't exist, SQLite will create it automatically. The cursor object allows us to execute SQL commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d7470",
   "metadata": {},
   "source": [
    "### Drop existing tables to ensure clean schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71228950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x728f5b413ec0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('DROP TABLE IF EXISTS commits')\n",
    "cursor.execute('DROP TABLE IF EXISTS repositories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76844e",
   "metadata": {},
   "source": [
    "**Clean Database Reset:**\n",
    "\n",
    "These SQL commands ensure a fresh start by removing any existing tables:\n",
    "\n",
    "- **DROP TABLE IF EXISTS**: Safely removes tables without errors if they don't exist\n",
    "- **Order Matters**: Drops `commits` table first due to foreign key dependency on `repositories`\n",
    "- **Clean Slate**: Prevents schema conflicts or data inconsistencies from previous runs\n",
    "- **Idempotent**: Safe to run multiple times without side effects\n",
    "\n",
    "This approach ensures that each notebook run starts with a consistent, empty database schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe00cb",
   "metadata": {},
   "source": [
    "Clean slate approach: Drop any existing tables to ensure we start with a fresh schema. This prevents conflicts if you run the notebook multiple times or if the table structure has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7da15",
   "metadata": {},
   "source": [
    "###  Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456b85e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x728f5b413ec0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS repositories (\n",
    "        repository_url TEXT PRIMARY KEY,\n",
    "        repository_name TEXT NOT NULL\n",
    "    )\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47878c",
   "metadata": {},
   "source": [
    "**Repositories Table Schema:**\n",
    "\n",
    "This creates the parent table for storing repository information:\n",
    "\n",
    "**Table Structure:**\n",
    "- **repository_url**: Primary key, unique identifier for each repository\n",
    "- **repository_name**: Human-readable name of the repository\n",
    "- **Data Type**: TEXT fields for string data\n",
    "- **Constraints**: PRIMARY KEY ensures uniqueness, NOT NULL prevents empty values\n",
    "\n",
    "**Purpose:**\n",
    "- Normalizes data to avoid redundancy\n",
    "- Establishes the parent entity in a one-to-many relationship with commits\n",
    "- Provides a clean separation between repository metadata and commit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd631dd",
   "metadata": {},
   "source": [
    "Create the `repositories` table to store unique repository information:\n",
    "- `repository_url`: Primary key, the unique URL of the repository\n",
    "- `repository_name`: Human-readable name of the repository\n",
    "\n",
    "This table normalizes the data to avoid storing repository info redundantly for each commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bb69cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x728f5b413ec0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS commits (\n",
    "        hash TEXT NOT NULL,\n",
    "        author TEXT NOT NULL,\n",
    "        date TEXT NOT NULL,\n",
    "        message TEXT NOT NULL,\n",
    "        repository_url TEXT NOT NULL,\n",
    "        PRIMARY KEY (repository_url, hash),\n",
    "        FOREIGN KEY (repository_url) REFERENCES repositories (repository_url)\n",
    "    )\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c3f79",
   "metadata": {},
   "source": [
    "**Commits Table Schema:**\n",
    "\n",
    "This creates the main table for storing detailed commit information:\n",
    "\n",
    "**Table Structure:**\n",
    "- **hash**: The unique Git commit SHA identifier\n",
    "- **author**: Developer who made the commit\n",
    "- **date**: Timestamp when the commit was made\n",
    "- **message**: Full commit message content\n",
    "- **repository_url**: Links to the repositories table (foreign key)\n",
    "\n",
    "**Key Constraints:**\n",
    "- **Composite Primary Key**: (repository_url, hash) ensures uniqueness across repos\n",
    "- **Foreign Key**: Links commits to their respective repositories\n",
    "- **NOT NULL**: All fields are required for data integrity\n",
    "\n",
    "This schema supports querying commits by repository, author, date ranges, or message content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eceaa8",
   "metadata": {},
   "source": [
    "Create the `commits` table to store commit information:\n",
    "- `hash`: The unique commit hash/SHA\n",
    "- `author`: Who made the commit\n",
    "- `date`: When the commit was made\n",
    "- `message`: The commit message content\n",
    "- `repository_url`: Foreign key linking to the repositories table\n",
    "\n",
    "The composite primary key (repository_url, hash) ensures uniqueness across repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18469614",
   "metadata": {},
   "source": [
    "### Insert data from the commits list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a165d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for commit in commits:\n",
    "    # Insert repository\n",
    "    cursor.execute(\n",
    "        'INSERT OR IGNORE INTO repositories (repository_url, repository_name) VALUES (?, ?)',\n",
    "        (commit['repo_url'], commit['repo_name'])\n",
    "    )\n",
    "    \n",
    "    # Insert commit\n",
    "    cursor.execute(\n",
    "        'INSERT OR IGNORE INTO commits (hash, author, date, message, repository_url) VALUES (?, ?, ?, ?, ?)',\n",
    "        (commit['hash'], commit['author'], commit['date'], commit['message'], commit['repo_url'])\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4c1e2",
   "metadata": {},
   "source": [
    "**Data Population Process:**\n",
    "\n",
    "This code populates the database with all parsed commit data:\n",
    "\n",
    "**Two-Step Insertion:**\n",
    "1. **Repository Insert**: Adds repository metadata first (parent table)\n",
    "2. **Commit Insert**: Adds commit details with foreign key reference\n",
    "\n",
    "**Key Features:**\n",
    "- **INSERT OR IGNORE**: Prevents duplicate entries and avoids errors\n",
    "- **Referential Integrity**: Maintains proper foreign key relationships\n",
    "- **Batch Processing**: Processes all commits in a single transaction\n",
    "- **Data Consistency**: Ensures all commits have valid repository references\n",
    "\n",
    "**Final Steps:**\n",
    "- **commit()**: Saves all changes to the database file\n",
    "- **close()**: Properly closes the database connection and frees resources\n",
    "\n",
    "This creates a fully populated, normalized database ready for complex queries and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e60399",
   "metadata": {},
   "source": [
    "Insert all the parsed commit data into the database:\n",
    "\n",
    "1. **Repository insertion**: Use `INSERT OR IGNORE` to add repositories without duplicates\n",
    "2. **Commit insertion**: Insert each commit with its metadata, linking to the repository\n",
    "3. **Transaction commit**: Save all changes to the database file\n",
    "4. **Connection cleanup**: Close the database connection to free resources\n",
    "\n",
    "The `OR IGNORE` clause prevents errors if we try to insert duplicate data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
